{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91406e-0328-4117-8cd8-cc3a356992cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import spacy\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "import re\n",
    "from openpyxl import Workbook\n",
    "\n",
    "nlp = spacy.load('models/en_core_web_sm/en_core_web_sm-3.8.0', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230dc43b-ac1f-42eb-83a1-9b5b3187630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "data_path = \"data\"\n",
    "\n",
    "for year in range(2019, 2025):\n",
    "    \n",
    "    # Loop through each file\n",
    "    for json_path in glob.glob(f\"{data_path}/{year}/*.json\"):\n",
    "        \n",
    "        # Open the file and append rows to master list \n",
    "        with open(json_path, \"r\") as f:\n",
    "            rows.append(json.load(f))\n",
    "            \n",
    "# Convert rows to df \n",
    "grants_df = pd.DataFrame(rows)\n",
    "grants_df[\"awd_id\"] = grants_df[\"awd_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of NSF-terminated grants \n",
    "terminated_awards = pd.read_csv(f'{data_path}/NSF-Terminated-Awards.csv', encoding='latin1',)\n",
    "terminated_awards = terminated_awards.rename(columns={\"Award ID\": \"awd_id\"})[[\"awd_id\"]]\n",
    "terminated_awards[\"awd_id\"] = terminated_awards[\"awd_id\"].astype(str)\n",
    "terminated_awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e81fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify whether grants are terminated or not \n",
    "grants_df['terminated'] = grants_df['awd_id'].isin(terminated_awards['awd_id']).astype(int)\n",
    "grants_df[\"terminated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both abstract cols into one value to extract keywords\n",
    "print(grants_df[[\"abst_narr_txt\", \"awd_abstract_narration\"]].isna().sum())\n",
    "\n",
    "def create_abstract_col(row):\n",
    "    abstract = []\n",
    "    if pd.notna(row[\"abst_narr_txt\"]):\n",
    "        abstract.append(row[\"abst_narr_txt\"])\n",
    "    if pd.notna(row[\"awd_abstract_narration\"]):\n",
    "        abstract.append(row[\"awd_abstract_narration\"])\n",
    "        \n",
    "    abstract = \"; \".join(abstract) if abstract else \"\"\n",
    "\n",
    "    # Remove the NSF mission statement from the abstract\n",
    "    abstract = abstract.replace(\"This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.\", \"\")\n",
    "\n",
    "    return abstract \n",
    "\n",
    "grants_df['abstract'] = grants_df.apply(create_abstract_col, axis=1)\n",
    "grants_df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSF Division Name Mapping\n",
    "division_names = {\n",
    "    # Mathematical & Physical Sciences\n",
    "    'DMS': 'Mathematical Sciences',\n",
    "    'PHY': 'Physics',\n",
    "    'CHE': 'Chemistry',\n",
    "    'DMR': 'Materials Research',\n",
    "    'AST': 'Astronomical Sciences',\n",
    "    \n",
    "    # Computer & Information Science\n",
    "    'CNS': 'Computer and Network Systems',\n",
    "    'IIS': 'Information & Intelligent Systems',\n",
    "    'CCF': 'Computing and Communication Foundations',\n",
    "    'OAC': 'Advanced Cyberinfrastructure',\n",
    "    \n",
    "    # Engineering\n",
    "    'CBET': 'Chemical, Bioengineering, Environmental, and Transport Systems',\n",
    "    'CMMI': 'Civil, Mechanical and Manufacturing Innovation',\n",
    "    'ECCS': 'Electrical, Communications and Cyber Systems',\n",
    "    'EEC': 'Engineering Education and Centers',\n",
    "    \n",
    "    # Biological Sciences\n",
    "    'DEB': 'Environmental Biology',\n",
    "    'IOS': 'Integrative Organismal Systems',\n",
    "    'DBI': 'Biological Infrastructure',\n",
    "    'MCB': 'Molecular and Cellular Biosciences',\n",
    "    \n",
    "    # Geosciences\n",
    "    'EAR': 'Earth Sciences',\n",
    "    'OCE': 'Ocean Sciences',\n",
    "    'AGS': 'Atmospheric and Geospace Sciences',\n",
    "    \n",
    "    # Social, Behavioral & Economic Sciences\n",
    "    'BCS': 'Behavioral and Cognitive Sciences',\n",
    "    'SES': 'Social and Economic Sciences',\n",
    "    'SMA': 'SBE Multidisciplinary Activities',\n",
    "    \n",
    "    # Education & Human Resources\n",
    "    'DUE': 'Undergraduate Education',\n",
    "    'DRL': 'Research on Learning',\n",
    "    'DGE': 'Graduate Education',\n",
    "    'EES': 'EPSCoR',\n",
    "    \n",
    "    # Technology & Innovation\n",
    "    'TI': 'Technology Innovation',\n",
    "    'RISE': 'Research on Innovative Technologies',\n",
    "    'ITE': 'Innovation and Technology Ecosystems',\n",
    "    \n",
    "    # Office Programs\n",
    "    'OPP': 'Polar Programs',\n",
    "    'OIA': 'Integrative Activities',\n",
    "    'OISE': 'International Science and Engineering',\n",
    "    'OSI': 'Strategic Initiatives',\n",
    "    \n",
    "    # Emerging/Multidisciplinary\n",
    "    'EFMA': 'Emerging Frontiers & Multidisciplinary Activities',\n",
    "    'EF': 'Emerging Frontiers',\n",
    "    \n",
    "    # Administrative/Other\n",
    "    'DAS': 'Division of Acquisition and Cooperative Support',\n",
    "    'BFA': 'Budget, Finance and Award Management',\n",
    "    'HRM': 'Human Resource Management',\n",
    "    'NCSE': 'National Center for Science and Engineering Statistics',\n",
    "    'TF': 'The NSF Trust Fund',\n",
    "    'OIG': 'Office of Inspector General',\n",
    "    'OGC': 'Office of General Counsel',\n",
    "    'O/D': 'Office of Director',\n",
    "    'RIO': 'Research Infrastructure Office',\n",
    "    'OCR': 'Office of Civil Rights',\n",
    "    'DES': 'Division of Earth Sciences (alternate)',\n",
    "    'DOB': 'Division of Ocean Sciences (alternate)',\n",
    "    'DIS': 'Division of Information Systems',\n",
    "    'CRSP': 'Collaborative Research Support Program',\n",
    "    'NSB': 'National Science Board',\n",
    "    'NCO': 'National Coordination Office',\n",
    "    'LPA': 'Legislative and Public Affairs',\n",
    "    'IRM': 'Information and Resource Management',\n",
    "    'NNCO': 'National Nanotechnology Coordination Office'\n",
    "}\n",
    "\n",
    "# Apply mapping to grants df\n",
    "grants_df['division_name'] = grants_df['div_abbr'].map(division_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609284be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bigram and trigram models\n",
    "\n",
    "# Load stopwords from models folder (downloaded from NLTK)\n",
    "stopwords_file = 'models/stopwords/english'\n",
    "with open(stopwords_file, 'r') as f:\n",
    "    words = []\n",
    "    for line in f:\n",
    "        words.append(line.strip())\n",
    "    stop_words = set(words)\n",
    "\n",
    "# Loop through each row in the df, tokenize the abstract and store in the df\n",
    "grants_df[\"tokenized_abstract\"] = \"\"\n",
    "data = []\n",
    "for i, row in grants_df.iterrows():\n",
    "    tokens = [word for word in simple_preprocess(str(row[\"abstract\"]), deacc=True, min_len=3) \n",
    "             if word not in stop_words]\n",
    "    grants_df.at[i, 'tokenized_abstract'] = tokens  \n",
    "    \n",
    "    # Also add to data list for model training below\n",
    "    data.append(tokens)  \n",
    "\n",
    "# Train models on tokenized data\n",
    "bigram = gensim.models.Phrases(data, min_count=20, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data], threshold=100)\n",
    "\n",
    "# Create phrasers\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keyphrases from abstracts\n",
    "\n",
    "def process_words(row, stop_words=stop_words, allowed_tags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatization\n",
    "    \"\"\"\n",
    "        \n",
    "    tokens = row[\"tokenized_abstract\"]\n",
    "    \n",
    "    # Apply bigram and trigram models to create phrases\n",
    "    tokens = bigram_mod[tokens]\n",
    "    tokens = trigram_mod[bigram_mod[tokens]]  \n",
    "    \n",
    "    # Separate phrases (with underscores) from single words\n",
    "    phrases = []\n",
    "    single_words = []\n",
    "    for token in tokens:\n",
    "        if '_' in token:\n",
    "            phrases.append(token)\n",
    "        else:\n",
    "            single_words.append(token)\n",
    "    \n",
    "    # Lemmatize single words only (to not remove underscores)\n",
    "    if single_words:\n",
    "        doc = nlp(\" \".join(single_words))\n",
    "        lemmatized = [token.lemma_ for token in doc if token.pos_ in allowed_tags]\n",
    "    else:\n",
    "        lemmatized = []\n",
    "    \n",
    "    # Combine phrases and lemmatized words, filter stopwords and short tokens\n",
    "    result = phrases + lemmatized\n",
    "    return [word for word in result if word not in stop_words and len(word) >= 3]\n",
    "\n",
    "grants_df[\"keyphrases\"] = grants_df.apply(process_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train separate LDA models for each division\n",
    "\n",
    "# Dictionary to store models and metadata for each division\n",
    "division_models = {}\n",
    "\n",
    "# LDA parameters\n",
    "num_topics = 10\n",
    "alpha = 0.1\n",
    "eta = 0.05\n",
    "\n",
    "for division in grants_df['division_name'].unique():\n",
    "    print(f\"\\nProcessing: {division}\")\n",
    "    \n",
    "    # Filter dataframe for this division\n",
    "    div_df = grants_df[grants_df['division_name'] == division].copy()\n",
    "    \n",
    "    # Filter out abstracts (or lack thereof) with no keyphrases\n",
    "    div_df = div_df.dropna(subset=['keyphrases'])\n",
    "    \n",
    "    print(f\"  Number of grants: {len(div_df)}\")\n",
    "    \n",
    "    # Skip divisions with less than 50 grants\n",
    "    if len(div_df) < 50:\n",
    "        print(f\"  Skipping {division}: only has {len(div_df)} total grants\")\n",
    "        continue\n",
    "    \n",
    "    # Get token lists for this division\n",
    "    token_lists = div_df[\"keyphrases\"].tolist()\n",
    "    \n",
    "    # Create dictionary for this division\n",
    "    id2word = corpora.Dictionary(token_lists)\n",
    "    \n",
    "    # Filter extremes: remove words that appear in <2 documents or >50% of documents\n",
    "    id2word.filter_extremes(no_below=2, no_above=0.5)\n",
    "    print(f\"  Corpus size: {len(id2word)}\")\n",
    "    \n",
    "    if len(id2word) < 10:\n",
    "        print(f\"  Skipping {division}: insufficient vocabulary ({len(id2word)} < 10)\")\n",
    "        continue\n",
    "    \n",
    "    # Create corpus for this division\n",
    "    corpus = [id2word.doc2bow(text) for text in token_lists]\n",
    "    \n",
    "    # Train LDA model\n",
    "    print(f\"  Training LDA model...\")\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        random_state=100,\n",
    "        passes=50,\n",
    "        iterations=400,\n",
    "        per_word_topics=True,\n",
    "        eval_every=10\n",
    "    )\n",
    "    \n",
    "    # Store model and metadata\n",
    "    division_models[division] = {\n",
    "        'model': lda_model,\n",
    "        'id2word': id2word,\n",
    "        'corpus': corpus,\n",
    "        'dataframe': div_df,\n",
    "        'num_grants': len(div_df),\n",
    "        'vocab_size': len(id2word)\n",
    "    }\n",
    "    \n",
    "    # Print topics for this division\n",
    "    print(f\"\\n  Topics for {division}:\")\n",
    "    for i, topic in lda_model.print_topics(-1, num_words=10):\n",
    "        print(f\"    Topic {i+1}: {topic}\")\n",
    "\n",
    "print(f\"\\n\\nCompleted! Trained models for {len(division_models)} divisions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export topics to Excel file with one sheet per division\n",
    "excel_file = 'lda_topics_by_division.xlsx'\n",
    "\n",
    "# Create Excel writer\n",
    "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "    \n",
    "    for division, model_data in division_models.items():\n",
    "        lda_model = model_data['model']\n",
    "        \n",
    "        # Extract topics and their words\n",
    "        topics_data = []\n",
    "        \n",
    "        # Get all topics for this division\n",
    "        for topic_id in range(lda_model.num_topics):\n",
    "            \n",
    "            # Get topic words with probabilities of each token\n",
    "            topic_words = lda_model.show_topic(topic_id, topn=20)  # Get top 20 words\n",
    "            \n",
    "            # Create a row for this topic\n",
    "            topic_row = {'Topic': f'Topic {topic_id + 1}'}\n",
    "            \n",
    "            # Add words and probabilities\n",
    "            for rank, (token, _) in enumerate(topic_words):\n",
    "                topic_row[f'Word {rank+1}'] = token\n",
    "            \n",
    "            topics_data.append(topic_row)\n",
    "        \n",
    "        # Create DataFrame for this division and write to Excel\n",
    "        df_topics = pd.DataFrame(topics_data)\n",
    "        df_topics.to_excel(writer, sheet_name=division[:31], index=False)\n",
    "        \n",
    "        print(f\"Exported topics for: {division}\")\n",
    "\n",
    "print(f\"\\nExcel file created: {excel_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsf (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
