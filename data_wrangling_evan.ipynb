{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91406e-0328-4117-8cd8-cc3a356992cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import spacy\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "nlp = spacy.load('models/en_core_web_sm/en_core_web_sm-3.8.0', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230dc43b-ac1f-42eb-83a1-9b5b3187630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "data_path = \"data\"\n",
    "\n",
    "for year in range(2019, 2025):\n",
    "    \n",
    "    # Loop through each file\n",
    "    for json_path in glob.glob(f\"{data_path}/{year}/*.json\"):\n",
    "        \n",
    "        # Open the file and append rows to master list \n",
    "        with open(json_path, \"r\") as f:\n",
    "            rows.append(json.load(f))\n",
    "            \n",
    "# Convert rows to df \n",
    "grants_df = pd.DataFrame(rows)\n",
    "grants_df[\"awd_id\"] = grants_df[\"awd_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597a418e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awd_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1231319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1432910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1661201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1712692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1723165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>2510215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>2513528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>2514823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>2516400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>2520318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       awd_id\n",
       "0     1231319\n",
       "1     1432910\n",
       "2     1661201\n",
       "3     1712692\n",
       "4     1723165\n",
       "...       ...\n",
       "1662  2510215\n",
       "1663  2513528\n",
       "1664  2514823\n",
       "1665  2516400\n",
       "1666  2520318\n",
       "\n",
       "[1667 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of NSF-terminated grants \n",
    "terminated_awards = pd.read_csv(f'{data_path}/NSF-Terminated-Awards.csv', encoding='latin1',)\n",
    "terminated_awards = terminated_awards.rename(columns={\"Award ID\": \"awd_id\"})[[\"awd_id\"]]\n",
    "terminated_awards[\"awd_id\"] = terminated_awards[\"awd_id\"].astype(str)\n",
    "terminated_awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e81fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "73003    0\n",
       "73004    0\n",
       "73005    1\n",
       "73006    0\n",
       "73007    0\n",
       "Name: terminated, Length: 73008, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify whether grants are terminated or not \n",
    "grants_df['terminated'] = grants_df['awd_id'].isin(terminated_awards['awd_id']).astype(int)\n",
    "grants_df[\"terminated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec5013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abst_narr_txt             72998\n",
      "awd_abstract_narration      324\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        The broader impact/commercial potential of thi...\n",
       "1        The membrane of a cell separates the cell's in...\n",
       "2        The broader impact/commercial potential of thi...\n",
       "3        Arthropod parasites (specifically, insects and...\n",
       "4        The 31st Cumberland Conference on Combinatoric...\n",
       "                               ...                        \n",
       "73003    The analysis of images has been used by the sc...\n",
       "73004    Terrestrial land surfaces rise above the ocean...\n",
       "73005    A strong workforce in emerging technology area...\n",
       "73006    This project plans to enable a network instrum...\n",
       "73007    Visioning is used by the research communities ...\n",
       "Name: abstract, Length: 73008, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine both abstract cols into one value to extract keywords\n",
    "print(grants_df[[\"abst_narr_txt\", \"awd_abstract_narration\"]].isna().sum())\n",
    "\n",
    "def create_abstract_col(row):\n",
    "    abstract = []\n",
    "    if pd.notna(row[\"abst_narr_txt\"]):\n",
    "        abstract.append(row[\"abst_narr_txt\"])\n",
    "    if pd.notna(row[\"awd_abstract_narration\"]):\n",
    "        abstract.append(row[\"awd_abstract_narration\"])\n",
    "        \n",
    "    abstract = \"; \".join(abstract) if abstract else \"\"\n",
    "\n",
    "    # Remove the NSF mission statement from the abstract\n",
    "    abstract = abstract.replace(\"This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.\", \"\")\n",
    "\n",
    "    return abstract \n",
    "\n",
    "grants_df['abstract'] = grants_df.apply(create_abstract_col, axis=1)\n",
    "grants_df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8893749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSF Division Name Mapping\n",
    "division_names = {\n",
    "    # Mathematical & Physical Sciences\n",
    "    'DMS': 'Mathematical Sciences',\n",
    "    'PHY': 'Physics',\n",
    "    'CHE': 'Chemistry',\n",
    "    'DMR': 'Materials Research',\n",
    "    'AST': 'Astronomical Sciences',\n",
    "    \n",
    "    # Computer & Information Science\n",
    "    'CNS': 'Computer and Network Systems',\n",
    "    'IIS': 'Information & Intelligent Systems',\n",
    "    'CCF': 'Computing and Communication Foundations',\n",
    "    'OAC': 'Advanced Cyberinfrastructure',\n",
    "    \n",
    "    # Engineering\n",
    "    'CBET': 'Chemical, Bioengineering, Environmental, and Transport Systems',\n",
    "    'CMMI': 'Civil, Mechanical and Manufacturing Innovation',\n",
    "    'ECCS': 'Electrical, Communications and Cyber Systems',\n",
    "    'EEC': 'Engineering Education and Centers',\n",
    "    \n",
    "    # Biological Sciences\n",
    "    'DEB': 'Environmental Biology',\n",
    "    'IOS': 'Integrative Organismal Systems',\n",
    "    'DBI': 'Biological Infrastructure',\n",
    "    'MCB': 'Molecular and Cellular Biosciences',\n",
    "    \n",
    "    # Geosciences\n",
    "    'EAR': 'Earth Sciences',\n",
    "    'OCE': 'Ocean Sciences',\n",
    "    'AGS': 'Atmospheric and Geospace Sciences',\n",
    "    \n",
    "    # Social, Behavioral & Economic Sciences\n",
    "    'BCS': 'Behavioral and Cognitive Sciences',\n",
    "    'SES': 'Social and Economic Sciences',\n",
    "    'SMA': 'SBE Multidisciplinary Activities',\n",
    "    \n",
    "    # Education & Human Resources\n",
    "    'DUE': 'Undergraduate Education',\n",
    "    'DRL': 'Research on Learning',\n",
    "    'DGE': 'Graduate Education',\n",
    "    'EES': 'EPSCoR',\n",
    "    \n",
    "    # Technology & Innovation\n",
    "    'TI': 'Technology Innovation',\n",
    "    'RISE': 'Research on Innovative Technologies',\n",
    "    'ITE': 'Innovation and Technology Ecosystems',\n",
    "    \n",
    "    # Office Programs\n",
    "    'OPP': 'Polar Programs',\n",
    "    'OIA': 'Integrative Activities',\n",
    "    'OISE': 'International Science and Engineering',\n",
    "    'OSI': 'Strategic Initiatives',\n",
    "    \n",
    "    # Emerging/Multidisciplinary\n",
    "    'EFMA': 'Emerging Frontiers & Multidisciplinary Activities',\n",
    "    'EF': 'Emerging Frontiers',\n",
    "    \n",
    "    # Administrative/Other\n",
    "    'DAS': 'Division of Acquisition and Cooperative Support',\n",
    "    'BFA': 'Budget, Finance and Award Management',\n",
    "    'HRM': 'Human Resource Management',\n",
    "    'NCSE': 'National Center for Science and Engineering Statistics',\n",
    "    'TF': 'The NSF Trust Fund',\n",
    "    'OIG': 'Office of Inspector General',\n",
    "    'OGC': 'Office of General Counsel',\n",
    "    'O/D': 'Office of Director',\n",
    "    'RIO': 'Research Infrastructure Office',\n",
    "    'OCR': 'Office of Civil Rights',\n",
    "    'DES': 'Division of Earth Sciences (alternate)',\n",
    "    'DOB': 'Division of Ocean Sciences (alternate)',\n",
    "    'DIS': 'Division of Information Systems',\n",
    "    'CRSP': 'Collaborative Research Support Program',\n",
    "    'NSB': 'National Science Board',\n",
    "    'NCO': 'National Coordination Office',\n",
    "    'LPA': 'Legislative and Public Affairs',\n",
    "    'IRM': 'Information and Resource Management',\n",
    "    'NNCO': 'National Nanotechnology Coordination Office'\n",
    "}\n",
    "\n",
    "# Apply mapping to grants df\n",
    "grants_df['division_name'] = grants_df['div_abbr'].map(division_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609284be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bigram and trigram models\n",
    "\n",
    "# Load stopwords from models folder (downloaded from NLTK)\n",
    "stopwords_file = 'models/stopwords/english'\n",
    "with open(stopwords_file, 'r') as f:\n",
    "    words = []\n",
    "    for line in f:\n",
    "        words.append(line.strip())\n",
    "    stop_words = set(words)\n",
    "\n",
    "# Loop through each row in the df, tokenize the abstract and store in the df\n",
    "grants_df[\"tokenized_abstract\"] = \"\"\n",
    "data = []\n",
    "for i, row in grants_df.iterrows():\n",
    "    tokens = [word for word in simple_preprocess(str(row[\"abstract\"]), deacc=True, min_len=3) \n",
    "             if word not in stop_words]\n",
    "    grants_df.at[i, 'tokenized_abstract'] = tokens  \n",
    "    \n",
    "    # Also add to data list for model training below\n",
    "    data.append(tokens)  \n",
    "\n",
    "# Train models on tokenized data\n",
    "bigram = gensim.models.Phrases(data, min_count=20, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data], threshold=100)\n",
    "\n",
    "# Create phrasers\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keyphrases from abstracts\n",
    "\n",
    "def process_words(row, stop_words=stop_words, allowed_tags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatization\n",
    "    \"\"\"\n",
    "        \n",
    "    tokens = row[\"tokenized_abstract\"]\n",
    "    \n",
    "    # Apply bigram and trigram models to create phrases\n",
    "    tokens = bigram_mod[tokens]\n",
    "    tokens = trigram_mod[bigram_mod[tokens]]  \n",
    "    \n",
    "    # Separate phrases (with underscores) from single words\n",
    "    phrases = []\n",
    "    single_words = []\n",
    "    for token in tokens:\n",
    "        if '_' in token:\n",
    "            phrases.append(token)\n",
    "        else:\n",
    "            single_words.append(token)\n",
    "    \n",
    "    # Lemmatize single words only (to not remove underscores)\n",
    "    if single_words:\n",
    "        doc = nlp(\" \".join(single_words))\n",
    "        lemmatized = [token.lemma_ for token in doc if token.pos_ in allowed_tags]\n",
    "    else:\n",
    "        lemmatized = []\n",
    "    \n",
    "    # Combine phrases and lemmatized words, filter stopwords and short tokens\n",
    "    result = phrases + lemmatized\n",
    "    return [word for word in result if word not in stop_words and len(word) >= 3]\n",
    "\n",
    "grants_df[\"keyphrases\"] = grants_df.apply(process_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "def lda_model(corpus, id2word, num_topics=5, eta=0.01, alpha=0.1):\n",
    "\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics,\n",
    "        alpha=alpha,\n",
    "        eta=eta, \n",
    "        random_state=42,\n",
    "        passes=50,  \n",
    "        iterations=400, \n",
    "        per_word_topics=True,\n",
    "        eval_every=10\n",
    "    )\n",
    "\n",
    "    # Print topics with more words to see patterns\n",
    "    print(\"Topics:\")\n",
    "    for i, topic in lda_model.print_topics(-1, num_words=20): # print all topics, 20 words per topic\n",
    "        print(f\"\\n   Topic {i}:\")\n",
    "        print(f\"   {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train separate LDA models for each division\n",
    "\n",
    "# Dictionary to store models and metadata for each division\n",
    "division_models = {}\n",
    "\n",
    "for division in grants_df['division_name'].unique():\n",
    "    print(f\"\\nProcessing: {division}\")\n",
    "    \n",
    "    # Filter dataframe for this division\n",
    "    div_df = grants_df[grants_df['division_name'] == division].copy()\n",
    "    \n",
    "    # Filter out abstracts (or lack thereof) with no keyphrases\n",
    "    div_df = div_df.dropna(subset=['keyphrases'])\n",
    "    \n",
    "    print(f\"  Number of grants: {len(div_df)}\")\n",
    "    \n",
    "    # Skip divisions with less than 50 grants\n",
    "    if len(div_df) < 50:\n",
    "        print(f\"  Skipping {division}: only has {len(div_df)} total grants\")\n",
    "        continue\n",
    "    \n",
    "    # Get token lists for this division\n",
    "    token_lists = div_df[\"keyphrases\"].tolist()\n",
    "    \n",
    "    # Create dictionary for this division\n",
    "    id2word = corpora.Dictionary(token_lists)\n",
    "    \n",
    "    # Filter extremes: remove words that appear in <2 documents or >50% of documents\n",
    "    id2word.filter_extremes(no_below=2, no_above=0.5)\n",
    "    print(f\"  Corpus size: {len(id2word)}\")\n",
    "    \n",
    "    if len(id2word) < 10:\n",
    "        print(f\"  Skipping {division}: insufficient vocabulary ({len(id2word)} < 10)\")\n",
    "        continue\n",
    "    \n",
    "    # Create corpus for this division\n",
    "    corpus = [id2word.doc2bow(text) for text in token_lists]\n",
    "    \n",
    "    # Train LDA model\n",
    "    print(f\"  Training LDA model...\")\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        random_state=100,\n",
    "        passes=50,\n",
    "        iterations=400,\n",
    "        per_word_topics=True,\n",
    "        eval_every=10\n",
    "    )\n",
    "    \n",
    "    # Store model and metadata\n",
    "    division_models[division] = {\n",
    "        'model': lda_model,\n",
    "        'id2word': id2word,\n",
    "        'corpus': corpus,\n",
    "        'dataframe': div_df,\n",
    "        'num_grants': len(div_df),\n",
    "        'vocab_size': len(id2word)\n",
    "    }\n",
    "    \n",
    "    # Print topics for this division\n",
    "    print(f\"\\n  Topics for {division}:\")\n",
    "    for i, topic in lda_model.print_topics(-1, num_words=10):\n",
    "        print(f\"    Topic {i}: {topic}\")\n",
    "\n",
    "print(f\"\\n\\nCompleted! Trained models for {len(division_models)} divisions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
